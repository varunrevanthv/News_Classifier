Creation of the dataset
================

Raw dataset
-----------

The dataset used in this project is the **BBC News Raw Dataset**. It can be downloaded from:

<http://mlg.ucd.ie/datasets/bbc.html>

It consists of 2.225 documents from the BBC news website corresponding to stories in five topical areas from 2004 to 2005. These areas are:

-   Business
-   Entertainment
-   Politics
-   Sport
-   Tech

In the same webpage we can find another dataset (`BBCSport`), which consists of 737 documents from the BBC Sport website. However, in this project it hasn't been used.

In addition, a pre-processed dataset is also provided. This pre-processing includes stemming, stop-word removal and low term frequency filtering. Again, it has not been used. The **raw dataset** has been used instead.

The download file `bbc-fulltext.zip` contains, for each category, every article in a separate `.txt` file.

The aim of this document is to provide explanation on how this information has been assembled into a single dataframe containing the text, category and filename of each article.

Dataset pre-processing
----------------------

The raw files are placed in:

`0. Latest News Classifier/00. Raw dataset/BBC/bbc-fulltext/bbc`.

And the script developed to process them is:

-   R Project: `0. Latest News Classifier/00. Raw dataset/01. Dataset Creation/01. Dataset Creation.Rproj`.
-   R Script: `0. Latest News Classifier/00. Raw dataset/01. Dataset Creation/01. Dataset Creation.R`.

### Description of the process

First of all, the packages needed are installed or loaded, the environment data is cleaned and the paths are set:

``` r
# Installs
install.packages("readtext", dependencies=T)

# Imports
library(readtext)

# Cleaning environment data
rm(list = ls())

# Working directory
setwd('C:/Users/migue/Data Science/Master Data Science/KSCHOOL/9. TFM/0. Latest News Classifier/01. Dataset Creation')

# Path definition of the news archives
path <- 'C:/Users/migue/Data Science/Master Data Science/KSCHOOL/9. TFM/0. Latest News Classifier/00. Raw dataset/BBC/bbc-fulltext/bbc'
```

We'll obtain a summary of the number of files in each category for information purposes:

``` r
# List with the 5 categories
list_categories <- list.files(path=path)

# Save to dataset the number of files in each category folder
summary_categories <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(summary_categories) <- c('Category', 'Number_of_Files')

for (category in list_categories){
  category_path <- paste(path, category, sep='/')
  n_files <- length(list.files(path=category_path))
  
  summary_categories = rbind(summary_categories, data.frame('Category'=category, 'Number_of_Files'=n_files))
}

summary_categories
```

Then, we'll read every folder and create the final dataframe by reading the text in every file. This dataframe will contain the `doc_ic`, `text` and `category` columns.

``` r
# Read every folder and create the final dataframe
df_final <- data.frame(matrix(ncol = 3, nrow = 0))
colnames(df_final) <- c('doc_id', 'text', 'category')

for(category in list_categories){
  category_path <- paste(path, category, sep='/')

  df <- readtext(category_path)
  df["category"] = category
  
  df_final = rbind(df_final, df)
}

colnames(df_final) <- c('File_Name', 'Content', 'Category')

df_final <-
  df_final %>% 
  mutate(Complete_Filename = paste(File_Name, Category, sep='-'))
```

Finally, we'll save the dataset into an R image file, write the code for loading it if necessary and export it to csv for further treatment with python:

``` r
# Save dataset: .rda
save(df_final, file='Dataset.rda')

# Load dataset
load(file='Dataset.rda')

# Write csv to import to python
write.csv2(df_final,fileEncoding = 'utf8', "News_dataset.csv", row.names = FALSE)
```

### Output

The output dataframe `News_dataset.csv`contains:

-   `File_Name`: Original name of the article
-   `Content`: Content of the article
-   `Category`: Category of the article
-   `Complete_Filename`: Column created as `File_Name` + `Category`. The objective is to have an unique identifier for each article, since the article file names are repeated across categories.

It has 2.225 rows.
