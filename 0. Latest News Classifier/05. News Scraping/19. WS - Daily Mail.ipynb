{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping: Daily Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain list of news from the coverpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url definition\n",
    "url = \"https://www.dailymail.co.uk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Request\n",
    "r1 = requests.get(url)\n",
    "r1.status_code\n",
    "\n",
    "# We'll save in coverpage the cover page content\n",
    "coverpage = r1.content\n",
    "\n",
    "# Soup creation\n",
    "soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "# News identification\n",
    "coverpage_news = soup1.find_all('h2', class_='linkro-darkred')\n",
    "len(coverpage_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list in which every element is a news article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2 class=\"linkro-darkred\">\n",
       "    <a href=\"/femail/article-6552901/Charlotte-Casiraghi-calls-engagement-Dimitri-Rassam.html\" itemprop=\"url\">Grace Kelly's granddaughter Charlotte Casiraghi calls off her engagement to Dimitri Rassam just FOUR MONTHS after giving birth to their first child together</a>\n",
       "  </h2>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverpage_news[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's extract the text from the articles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll define the number of articles we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_articles = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "for n in np.arange(0, number_of_articles):\n",
    "        \n",
    "    # Getting the link of the article\n",
    "    link = url + coverpage_news[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('p', class_='mol-para-with-font')\n",
    "    \n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(body)):\n",
    "        paragraph = body[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    # Removing special characters\n",
    "    final_article = re.sub(\"\\\\xa0\", \"\", final_article)\n",
    "        \n",
    "    news_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put them into:\n",
    "* a dataset which will the input of the models (`df_features`)\n",
    "* a dataset with the title and the link (`df_show_info`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features\n",
    "df_features = pd.DataFrame(\n",
    "     {'Article Content': news_contents \n",
    "    })\n",
    "\n",
    "# df_show_info\n",
    "df_show_info = pd.DataFrame(\n",
    "    {'Article Title': list_titles,\n",
    "     'Article Link': list_links})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A pair of student drug dealers have been spare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abu Hamza's son is the suspect held in connect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The doorman stabbed in Mayfair on New Year's E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quite a lot of space on the internet has been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grace Kelly's granddaughterCharlotte Casiraghi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Article Content\n",
       "0  A pair of student drug dealers have been spare...\n",
       "1  Abu Hamza's son is the suspect held in connect...\n",
       "2  The doorman stabbed in Mayfair on New Year's E...\n",
       "3  Quite a lot of space on the internet has been ...\n",
       "4  Grace Kelly's granddaughterCharlotte Casiraghi..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Title</th>\n",
       "      <th>Article Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student drug dealers are SPARED jail after imp...</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-65535...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Revealed: Hate preacher Abu Hamza's son, 26, i...</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-65540...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doorman stabbed in Mayfair on New Year's Eve d...</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-65485...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Enjoy crispy hash browns, the freshest fries, ...</td>\n",
       "      <td>https://www.dailymail.co.uk/femail/food/articl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grace Kelly's granddaughter Charlotte Casiragh...</td>\n",
       "      <td>https://www.dailymail.co.uk/femail/article-655...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Article Title  \\\n",
       "0  Student drug dealers are SPARED jail after imp...   \n",
       "1  Revealed: Hate preacher Abu Hamza's son, 26, i...   \n",
       "2  Doorman stabbed in Mayfair on New Year's Eve d...   \n",
       "3  Enjoy crispy hash browns, the freshest fries, ...   \n",
       "4  Grace Kelly's granddaughter Charlotte Casiragh...   \n",
       "\n",
       "                                        Article Link  \n",
       "0  https://www.dailymail.co.uk/news/article-65535...  \n",
       "1  https://www.dailymail.co.uk/news/article-65540...  \n",
       "2  https://www.dailymail.co.uk/news/article-65485...  \n",
       "3  https://www.dailymail.co.uk/femail/food/articl...  \n",
       "4  https://www.dailymail.co.uk/femail/article-655...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_show_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in how much time the script takes to get the news because this will impact directly on user experience. For this, we'll put it all into a single function and then call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_dailymail():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://www.dailymail.co.uk\"\n",
    "    \n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "    r1.status_code\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h2', class_='linkro-darkred')\n",
    "    len(coverpage_news)\n",
    "    \n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = url + coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('p', class_='mol-para-with-font')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(body)):\n",
    "            paragraph = body[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "         # Removing special characters\n",
    "        final_article = re.sub(\"\\\\xa0\", \"\", final_article)\n",
    "        \n",
    "        news_contents.append(final_article)\n",
    "        \n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links,\n",
    "         'Newspaper': 'Daily Mail'})\n",
    "    \n",
    "    return (df_features, df_show_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time elapsed is 31.879444 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "x, y = get_news_dailymail()\n",
    "end =time.time()\n",
    "te = end-start\n",
    "print(\"The time elapsed is %f seconds\" %(te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really slow. We won't include in the app."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
